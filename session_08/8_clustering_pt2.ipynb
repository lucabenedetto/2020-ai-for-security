{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Clustering (part 2)\n",
    "\n",
    "In this notebook, we will introduce other clustering algorithms and the metrics that can be used for evaluating them.\n",
    "\n",
    "Specifically, we will look at the following clustering algorithms:\n",
    "- KMeans ([Documentation of KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)),\n",
    "- DBSCAN ([Documentation of DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#)),\n",
    "- hierarchical clustering ([Documentation of AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering))\n",
    "\n",
    "and at the following techniques for evaluating clustering models:\n",
    "- the \"elbow method\"\n",
    "\n",
    "The dataset we are going to use in the same *fraud detection* dataset which we have used in some of the previous sessions.\n",
    "In case you do not remember the details: it contains a list of transactions from an online retailer: for each transaction, we have several attributes, as well as a label indicating whether a transaction was fraudulent or not.\n",
    "\n",
    "Since we have the target labels, we might as well used some classification algorithm on this dataset (as we did in the 3rd session); but in this notebook we are going to focus exclusively on clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- [0. Imports](#0.)\n",
    "- [1. analysis of different clustering algorithms on artificial data](#1.)\n",
    "    - [1.1 Creation of the artificial blobs of data](#1.1)\n",
    "    - [1.2 Analysis of the clustering algorithms when using dafault parameters](#1.2)\n",
    "    - [1.3 Let's try to select the number of clusters](#1.3)\n",
    "    - [1.4 What parameters can we change with DBSCAN?](#1.4)\n",
    "    - [1.5 Experimenting with different distributions of artificial data](#1.5)\n",
    "- [2. Clustering on the fraud detection dataset](#2.)\n",
    "    - [2.1 Analysis of the dataset](#2.1)\n",
    "    - [2.2 Data preparation for clustering](#2.2)\n",
    "    - [2.3 Clustering](#2.3)\n",
    "    - [2.4 KMeans](#2.4)\n",
    "    - [2.5 Evaluating the KMeans algorithm](#2.5)\n",
    "    - [2.6 The elbow method](#2.6)\n",
    "    - [2.7 AgglomerativeClustering](#2.7)\n",
    "    - [2.8 DBSCAN](#2.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.\n",
    "## Imports\n",
    "\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import (\n",
    "    KMeans, \n",
    "    AgglomerativeClustering, \n",
    "    DBSCAN\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score,\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "## Analyzing different clustering algorithms on artificial data\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "\n",
    "### Creation of the artificial blobs of data\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "random_state = 170\n",
    "alpha=0.5\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(X[:, 0], X[:, 1], alpha=alpha)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "\n",
    "### Analysis of the clustering algorithms when using dafault parameters\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(22,6))\n",
    "\n",
    "y_pred = KMeans(random_state=random_state).fit_predict(X)\n",
    "scatter = ax[0].scatter(X[:, 0], X[:, 1], alpha=alpha, c=y_pred)\n",
    "ax[0].add_artist(ax[0].legend(*scatter.legend_elements(), title=\"Cluster\"))  # this line is to add a legend with the ID of each cluster\n",
    "ax[0].set_title(\"KMeans\")\n",
    "\n",
    "y_pred = DBSCAN().fit_predict(X)\n",
    "scatter = ax[1].scatter(X[:, 0], X[:, 1], alpha=alpha, c=y_pred)\n",
    "ax[1].add_artist(ax[1].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "ax[1].set_title(\"DBSCAN\")\n",
    "\n",
    "y_pred = AgglomerativeClustering().fit_predict(X)\n",
    "scatter = ax[2].scatter(X[:, 0], X[:, 1], alpha=alpha, c=y_pred)\n",
    "ax[2].add_artist(ax[2].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "ax[2].set_title(\"AgglomerativeClustering\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For `KMeans` and `AgglomerativeClustering` we can specify the desired number of clusters as an argument (`n_clusters`), we have a different number of clusters since the default value is different\n",
    "- For `DBSCAN`, we cannot explicitly define the number of clusters, it \"finds\" the number of clusters depending on other arguments\n",
    "- In `KMeans` and `AgglomerativeClustering` all the points are assigned to a cluster; in `DBSCAN` some points are assigned to cluster `-1`, which contains the \"noisy samples\" (thus should not be considered as a cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "\n",
    "### Let's try to select the number of clusters\n",
    "[Index](#Index)\n",
    "\n",
    "Let's leave DBSCAN aside for the moment, as it does not have a `n_clusters` attribute.\n",
    "\n",
    "First of all, I define a function for plotting KMeans and AgglomerativeClustering results, since I will have to plot them several times and in this way it is more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_KMeans_and_AgglomerativeClustering(X, n_clusters, random_state=random_state, alpha=alpha):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "    y_pred = KMeans(n_clusters=n_clusters, random_state=random_state).fit_predict(X)\n",
    "    scatter = ax[0].scatter(X[:, 0], X[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[0].add_artist(ax[0].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[0].set_title(\"KMeans(n_clusters=%d)\" % n_clusters)\n",
    "\n",
    "    y_pred = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(X)\n",
    "    scatter = ax[1].scatter(X[:, 0], X[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[1].add_artist(ax[1].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[1].set_title(\"AgglomerativeClustering(n_clusters=%d)\" % n_clusters)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the \"correct\" number of clusters (i.e. 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_KMeans_and_AgglomerativeClustering(X, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we change the number of clusters?\n",
    "We did this analysis for KMeans in the previous notebook, but now let's repeat it with AgglomerativeClustering as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_KMeans_and_AgglomerativeClustering(X, n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an interesting difference between the two plots! According to KMeans, some points which are very close to the top-center blob are actually marked as belonging to the same cluster as the bottom left blob.\n",
    "\n",
    "Why is that? We can answer that question by looking at the centers of the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], alpha=0.2, c=y_pred)\n",
    "ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, label='cluster centers')\n",
    "ax.add_artist(ax.legend(*scatter.legend_elements(), title=\"Cluster\", loc='lower right'))\n",
    "ax.set_title(\"KMeans(n_clusters=%d)\" % n_clusters)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_KMeans_and_AgglomerativeClustering(X, n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_KMeans_and_AgglomerativeClustering(X, n_clusters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can see that `KMeans` tends to create clusters with regular shapes, while `AgglomerativeClustering` clusters have boundaries which depend on the density of points in each area.\n",
    "\n",
    "Both models seem to distinguish pretty well between the different blobs, if we use `n_clusters` >= the number of blobs (i.e. they do not assign elements of different blobs to the same clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 \n",
    "### What parameters can we change with DBSCAN?\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's defin a function which I can use to plot the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_DBSCAN(dbscan, X):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    y_pred = dbscan.fit_predict(X)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax.add_artist(ax.legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax.set_title(\"DBSCAN\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_DBSCAN(DBSCAN(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the documentation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_DBSCAN(DBSCAN(eps=0.75), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_DBSCAN(DBSCAN(eps=0.9), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_DBSCAN(DBSCAN(eps=0.3), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "\n",
    "### Experimenting with different distributions of artificial data\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_transformed = np.dot(X, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=alpha)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(22,22))\n",
    "\n",
    "for idx, n_clusters in enumerate([2, 3, 5]):\n",
    "    y_pred = KMeans(n_clusters=n_clusters, random_state=random_state).fit_predict(X_transformed)\n",
    "    scatter = ax[idx][0].scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][0].add_artist(ax[idx][0].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][0].set_title(\"KMeans (n_clusters=%d)\" % n_clusters)\n",
    "\n",
    "for idx, eps in enumerate([0.3, 0.5, 0.8]):\n",
    "    y_pred = DBSCAN(eps=eps).fit_predict(X_transformed)\n",
    "    scatter = ax[idx][1].scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][1].add_artist(ax[idx][1].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][1].set_title(\"DBSCAN (eps = %.2f)\" % eps)\n",
    "\n",
    "for idx, n_clusters in enumerate([2, 3, 5]):\n",
    "    y_pred = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(X_transformed)\n",
    "    scatter = ax[idx][2].scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][2].add_artist(ax[idx][2].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][2].set_title(\"AgglomerativeClustering (n_clusters = %d)\" % n_clusters)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Different variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var, y_var = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(X_var[:, 0], X_var[:, 1], alpha=alpha)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(22,22))\n",
    "\n",
    "for idx, n_clusters in enumerate([2, 3, 5]):\n",
    "    y_pred = KMeans(n_clusters=n_clusters, random_state=random_state).fit_predict(X_var)\n",
    "    scatter = ax[idx][0].scatter(X_var[:, 0], X_var[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][0].add_artist(ax[idx][0].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][0].set_title(\"KMeans (n_clusters=%d)\" % n_clusters)\n",
    "\n",
    "for idx, eps in enumerate([0.3, 0.5, 0.8]):\n",
    "    y_pred = DBSCAN(eps=eps).fit_predict(X_var)\n",
    "    scatter = ax[idx][1].scatter(X_var[:, 0], X_var[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][1].add_artist(ax[idx][1].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][1].set_title(\"DBSCAN (eps = %.2f)\" % eps)\n",
    "\n",
    "for idx, n_clusters in enumerate([2, 3, 5]):\n",
    "    y_pred = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(X_var)\n",
    "    scatter = ax[idx][2].scatter(X_var[:, 0], X_var[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][2].add_artist(ax[idx][2].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][2].set_title(\"AgglomerativeClustering (n_clusters = %d)\" % n_clusters)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Unevenly sized blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(X_filtered[:, 0], X_filtered[:, 1], alpha=alpha)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(22,22))\n",
    "\n",
    "for idx, n_clusters in enumerate([2, 3, 5]):\n",
    "    y_pred = KMeans(n_clusters=n_clusters, random_state=random_state).fit_predict(X_filtered)\n",
    "    scatter = ax[idx][0].scatter(X_filtered[:, 0], X_filtered[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][0].add_artist(ax[idx][0].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][0].set_title(\"KMeans (n_clusters=%d)\" % n_clusters)\n",
    "\n",
    "for idx, eps in enumerate([0.3, 0.5, 0.8]):\n",
    "    y_pred = DBSCAN(eps=eps).fit_predict(X_filtered)\n",
    "    scatter = ax[idx][1].scatter(X_filtered[:, 0], X_filtered[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][1].add_artist(ax[idx][1].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][1].set_title(\"DBSCAN (eps = %.2f)\" % eps)\n",
    "\n",
    "for idx, n_clusters in enumerate([2, 3, 5]):\n",
    "    y_pred = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(X_filtered)\n",
    "    scatter = ax[idx][2].scatter(X_filtered[:, 0], X_filtered[:, 1], alpha=alpha, c=y_pred)\n",
    "    ax[idx][2].add_artist(ax[idx][2].legend(*scatter.legend_elements(), title=\"Cluster\"))\n",
    "    ax[idx][2].set_title(\"AgglomerativeClustering (n_clusters = %d)\" % n_clusters)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "## Clustering on the fraud detection dataset\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "### Analysis of the dataset\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('payment_fraud.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: we often had to define in advance the column names. However, this time we didn't have to do so. Why?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Display 1 random row of the dataframe.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Print the shape of the dataframe</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Some clustering algorithms (e.g. AgglomerativeClustering) require a lot of memory. In order not to have any problems in running them on my machine during this session, I subsample the original dataframe in order to have only few of the original samples.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign = df[df['label']==0]\n",
    "df_fraud = df[df['label']!=0]\n",
    "\n",
    "df = pd.concat([df_benign.sample(2000, random_state=random_state), df_fraud], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Show the type of each feature.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Display the number of occurrences of each value of 'paymentMethod'.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Plot the distribution of 'accountAgeDays'.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Plot the distribution of 'localTime'.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Plot the distribution of 'paymentMethodAgeDays'.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Plot 'paymentMethodAgeDays' versus 'accountAgeDays'. Can you see a relationship between them? If so, what is it? Also, try to do the same thing separating fraudulent and not fraudulent entries (e.g. by plotting them in different colors).</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: The plots above suggest the reason why this was a very easy dataset (if you remember, back in the 3rd session we got 100% accuracy on it!). What is such reason?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As in previous sessions, I drop the `AccountAgeDays` column, in order to create a more difficult dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('accountAgeDays', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Try to look for any correlations between 'paymentMethodAgeDays' and 'numItems'. Do that separating fraudulent and not fraudulent entries as well (e.g. by plotting them in different colors).</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "### Data preparation for clustering\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Create a new dataframe performing one hot encoding on the feature(s) that require so.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_one_hot = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Complete the cell below in order to perform scaling.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_cols = ...\n",
    "\n",
    "# standard_scaler = StandardScaler().fit...\n",
    "# df_one_hot[numeric_cols] = standard_scaler..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "### Clustering\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation:\n",
    "- [kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "- [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)\n",
    "- [hierarchical clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary**: I create a new dataframe which does not contain the true label, and an array which contains only the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df_one_hot['label'].values\n",
    "df = df_one_hot.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "### `KMeans`\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Define a <code>KMeans</code> object with 2 clusters and perform the clustering. Also, measure the time elapsed for training.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Print the coordinates of the center of each cluster.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "### Evaluating the `KMeans` algorithm\n",
    "[Index](#Index)\n",
    "\n",
    "There are many possible metrics for clustering evaluation: some can be used when the ground truth labels are known, some can be used when the true labels are unknown.\n",
    "\n",
    "**Extrinsic evaluation** - If the ground truth is known (as in this case, since we have the `label` column) you could use:\n",
    "- [homogeneity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score)\n",
    "- [completeness](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score)\n",
    "- [v-measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score)\n",
    "\n",
    "**Intrinsic evaluation** - If the ground truth is not known, you could use:\n",
    "- [silhouette](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) - \"a higher Silhouette Coefficient score relates to a model with better defined clusters\"\n",
    "- [Calinski-Harabasz (C-H) Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html) - \"The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Above, I have instered links to the documentation of each metric. Looking at the documentation and the examples provided here, try to compute the metrics for the KMenas clustering we performed.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extrinsic evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Homogeneity score  = %.4f\" % homogeneity_score(true_labels, predicted_clusters))\n",
    "# TODO: completeness_score\n",
    "# TODO: v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intrinsic evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette score  = %.4f\" % silhouette_score(df, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Print Calinshi Harabasz score.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Now perform k-means clustering for k=3, compute the metrics and compare the results.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit KMeans and evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6\n",
    "### The elbow method\n",
    "[Index](#Index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KMeans, how do you choos the best number of clusters?\n",
    "\n",
    "Looking at the metric only is not very helpful, as they tend to decrease as the number of cluster increases.\n",
    "We can use the elbow method: that is, we compute the evaluation metrics for different values of `n_clusters` and observe how the evaluation metrics change.\n",
    "Then, we pick as number of clusters the value that caused a \"step\" in the chose evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homogeneity score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_values = []\n",
    "x_range = range(2, 20) \n",
    "for k in x_range:\n",
    "    t0 = time.time()\n",
    "    kmeans = KMeans(n_clusters=k, random_state=random_state).fit(df)\n",
    "    homogeneity_values.append(homogeneity_score(true_labels, kmeans.labels_))\n",
    "    print(\"k = %2d; Elapsed time = %.2f s\" % (k, time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_range, homogeneity_values)\n",
    "ax.set_xticks(x_range)\n",
    "ax.set_title('KMEANS - Homogeneity for varying K')\n",
    "ax.set_xlabel('K')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calinski Harabasz Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_values = []\n",
    "for k in x_range:\n",
    "    t0 = time.time()\n",
    "    kmeans = KMeans(n_clusters=k, random_state=random_state).fit(df)\n",
    "    ch_values.append(calinski_harabasz_score(df, kmeans.labels_))\n",
    "    print(\"k = %2d; Elapsed time = %.2f s\" % (k, time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_range, ch_values)\n",
    "ax.set_xticks(x_range)\n",
    "ax.set_title('KMEANS - C-H Index for varying K')\n",
    "ax.set_xlabel('K')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Repeat the analysis with the elbow method using the silhouette score</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7\n",
    "### `AgglomerativeClustering`\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Repeat the analysis with the elbow method using the homogeneity score on AgglomerativeClustering.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Repeat the analysis with the elbow method using the homogeneity score on AgglomerativeClustering.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8\n",
    "### `DBSCAN`\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Repeat the analysis with the elbow method using the homogeneity score and C-H Index on DBSCAN. Remember that with DBSCAN you cannot select the n_cluster attribute, you have to select the eps attribute.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homogeneity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calinski Harabasz Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
