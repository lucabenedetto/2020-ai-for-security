{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Anomaly Detection\n",
    "\n",
    "In this session, we will have a look at some techniques which can be used for anomaly detection.\n",
    "\n",
    "We will work on two dataset:\n",
    "- a toy dataset, artificially generated with the sklearn methods\n",
    "- a real Credit Card dataset\n",
    "\n",
    "We will be working with four models:\n",
    "- OneClassSVM (`class sklearn.svm.OneClassSVM`)\n",
    "- EllipticEnvelope (`class sklearn.covariance.EllipticEnvelope`), a.k.a. Robust Covariance\n",
    "- IsolationForest (`class sklearn.ensemble.IsolationForest`)\n",
    "- Local Outlier Factor (`class sklearn.neighbors.LocalOutlierFactor`)\n",
    "\n",
    "## Index\n",
    "\n",
    "- [9.0 - Imports](#9.0)\n",
    "- [9.1 - Comparing anomaly detection algorithms for outlier detection on toy datasets](#9.1)\n",
    "- [9.2 - Anomaly detection in Credit Card usage data](#9.2)\n",
    "    - [9.2.1 - Visualization](#9.2.1)\n",
    "    - [9.2.2 - Anomaly Detection](#9.2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing and data preparation\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# anomaly detection\n",
    "from sklearn import svm\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1\n",
    "## Comparing anomaly detection algorithms for outlier detection on toy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows characteristics of different anomaly detection algorithms on 2D datasets.\n",
    "\n",
    "Datasets contain one or two modes (regions of high density) to illustrate the ability of algorithms to cope with multimodal data.\n",
    "\n",
    "For each dataset, 15% of samples are generated as random uniform noise. \n",
    "This proportion is the value given to the nu parameter of the OneClassSVM and the contamination parameter of the other outlier detection algorithms (which are therefore used in the \"optimal\" configuration; in reality, it is not easy to find the value of these parameters).\n",
    "\n",
    "Decision boundaries between inliers and outliers are displayed in black except for Local Outlier Factor (LOF) as it has no predict method to be applied on new data when it is used for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "# setting for the synthetic data\n",
    "n_samples = 300\n",
    "outliers_fraction = 0.15\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "n_inliers = n_samples - n_outliers\n",
    "\n",
    "print(\"n_samples  = %3d\" % n_samples)\n",
    "print(\"n_outliers = %3d\" % n_outliers)\n",
    "print(\"n_inliers  = %3d\" % n_inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define outlier/anomaly detection methods to be compared\n",
    "anomaly_algorithms = [\n",
    "    (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction)),\n",
    "    (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.1)),\n",
    "    (\"Isolation Forest\", IsolationForest(contamination=outliers_fraction, random_state=RANDOM_SEED)),\n",
    "    (\"Local Outlier Factor\", LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
    "datasets = [\n",
    "    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3], **blobs_params)[0],\n",
    "    4. * (make_moons(n_samples=n_inliers, noise=.05, random_state=RANDOM_SEED)[0] - np.array([0.5, 0.25])),\n",
    "    14. * (np.random.RandomState(RANDOM_SEED).rand(n_inliers, 2) - 0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(24, 4))\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    ax[idx].scatter(dataset[:,0], dataset[:,1], s=15, alpha=0.5, c='g')\n",
    "plt.show()\n",
    "# be careful with the axis, they are on different scales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_w_ouliers = []\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "for X in datasets:\n",
    "    datasets_w_ouliers.append(np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(24, 4))\n",
    "for idx, dataset in enumerate(datasets_w_ouliers):\n",
    "#     ax[idx].scatter(dataset[:,0], dataset[:,1], s=15, alpha=0.5, color=['g']*n_inliers+['r']*n_outliers)\n",
    "    ax[idx].scatter(dataset[:n_inliers,0], dataset[:n_inliers,1], s=15, alpha=0.5, color='g')\n",
    "    ax[idx].scatter(dataset[n_inliers:,0], dataset[n_inliers:,1], s=15, alpha=0.5, color='r')\n",
    "    ax[idx].set_xlim(-7.5, 7.5)\n",
    "    ax[idx].set_ylim(-7.5, 7.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we know which are the anomalies, and therefore we can plot them in a different color, to see where they are in the space.\n",
    "\n",
    "Let's now run (separately on each dataset) the anomaly detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 4, figsize=(19, 24))\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-7.5, 7.5, 150), np.linspace(-7.5, 7.5, 150))\n",
    "\n",
    "for idx1, X in enumerate(datasets_w_ouliers):\n",
    "\n",
    "    for idx2, (name, algorithm) in enumerate(anomaly_algorithms):\n",
    "        if idx1 == 0:\n",
    "            ax[idx1][idx2].set_title(name, size=12)\n",
    "\n",
    "        # fit the data and tag outliers\n",
    "        if name == \"Local Outlier Factor\":\n",
    "            y_pred = algorithm.fit_predict(X)\n",
    "        else:\n",
    "            y_pred = algorithm.fit(X).predict(X)\n",
    "\n",
    "        # plot the levels lines and the points\n",
    "        if name != \"Local Outlier Factor\":  # LOF does not implement predict\n",
    "            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax[idx1][idx2].contour(xx, yy, Z, levels=[0], linewidths=2, colors='black', alpha=0.5)\n",
    "\n",
    "        colors = np.array(['#377eb8', '#ff7f00'])\n",
    "        ax[idx1][idx2].scatter(X[:, 0], X[:, 1], s=15, color=colors[(y_pred + 1) // 2], alpha=0.5)\n",
    "\n",
    "        ax[idx1][idx2].set_xlim(-7.5, 7.5)\n",
    "        ax[idx1][idx2].set_ylim(-7.5, 7.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The :class:`sklearn.svm.OneClassSVM` is known to be sensitive to outliers and thus does not perform very well for outlier detection.\n",
    "This estimator is best suited for novelty detection when the training set is not contaminated by outliers. \n",
    "That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.\n",
    "\n",
    ":class:`sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and learns an ellipse. \n",
    "It thus degrades when the data is not unimodal. \n",
    "Notice however that this estimator is robust to outliers.\n",
    "\n",
    ":class:`sklearn.ensemble.IsolationForest` and :class:`sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well for multi-modal data sets. \n",
    "The advantage of :class:`sklearn.neighbors.LocalOutlierFactor` over the other estimators is shown for the third data set, where the two modes have different densities.\n",
    "This advantage is explained by the local aspect of LOF, meaning that it only compares the score of abnormality of one sample with the scores of its neighbors.\n",
    "\n",
    "Finally, for the last data set, it is hard to say that one sample is more abnormal than another sample as they are uniformly distributed in a hypercube. \n",
    "Except for the :class:`sklearn.svm.OneClassSVM` which overfits a little, all estimators present decent solutions for this situation. \n",
    "In such a case, it would be wise to look more closely at the scores of abnormality of the samples as a good estimator should assign similar scores to all the samples.\n",
    "\n",
    "While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.\n",
    "\n",
    "Finally, note that parameters of the models have been here handpicked but that in practice they need to be adjusted. \n",
    "In the absence of labelled data, the problem is completely unsupervised so model selection can be a challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of simple univariate analysis\n",
    "\n",
    "Another option to perform anomaly detection is to do some univariate analysis, such as computing the **median** and the **MAD** (Median Absolute Deviation) and labelling points more than `k` times MAD abeove the median as outliers (where `k` is a parameter, similar to the k of kMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(24, 4))\n",
    "\n",
    "for idx, X in enumerate(datasets_w_ouliers):\n",
    "    mad = stats.median_abs_deviation(X)\n",
    "    median = np.median(X, axis=0)\n",
    "    thres = (median - k*mad), (median + k*mad)\n",
    "    y_pred = [np.any([x < thres[0], x > thres[1]]) for x in X]\n",
    "    print(np.unique(y_pred, return_counts=True))\n",
    "    \n",
    "    ax[idx].scatter(X[:,0], X[:,1], s=15, alpha=0.5, color=['r' if pred else 'g' for pred in y_pred])\n",
    "    ax[idx].set_xlim(-7.5, 7.5)\n",
    "    ax[idx].set_ylim(-7.5, 7.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this approach is quite \"weak\", and it works well only if the data has a round-ish shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2\n",
    "## Anomaly detection in Credit Card usage data\n",
    "[Index](#Index)\n",
    "\n",
    "We will now analyse a credit card usage dataset, available on Kaggle: \n",
    "https://www.kaggle.com/arjunbhasin2013/ccdata\n",
    "\n",
    "We also uploaded the dataset to Beep.\n",
    "\n",
    "Follow the link to read about the fields in the dataset, here a short recap:\n",
    "- `CUSTID` : Identification of Credit Card holder (Categorical)\n",
    "- `BALANCE` : Balance amount left in their account to make purchases (\n",
    "- `BALANCEFREQUENCY` : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n",
    "- `PURCHASES` : Amount of purchases made from account\n",
    "- `ONEOFFPURCHASES` : Maximum purchase amount done in one-go\n",
    "- `INSTALLMENTSPURCHASES` : Amount of purchase done in installment\n",
    "- `CASHADVANCE` : Cash in advance given by the user\n",
    "- `PURCHASESFREQUENCY` : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n",
    "- `ONEOFFPURCHASESFREQUENCY` : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n",
    "- `PURCHASESINSTALLMENTSFREQUENCY` : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n",
    "- `CASHADVANCEFREQUENCY` : How frequently the cash in advance being paid\n",
    "- `CASHADVANCETRX` : Number of Transactions made with \"Cash in Advanced\"\n",
    "- `PURCHASESTRX` : Numbe of purchase transactions made\n",
    "- `CREDITLIMIT` : Limit of Credit Card for user\n",
    "- `PAYMENTS` : Amount of Payment done by user\n",
    "- `MINIMUM_PAYMENTS` : Minimum amount of payments made by user\n",
    "- `PRCFULLPAYMENT` : Percent of full payment paid by user\n",
    "- `TENURE` : Tenure of credit card service for user\n",
    "\n",
    "\n",
    "\n",
    "We will first visualise different aspects of the data and then apply some anomaly detection techniques to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1\n",
    "### Visualisation\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Read the data into a pandas dataframe named <code>df</code> and print out the first few lines to have a look at it.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: How many rows are there in the data?.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: What are the types of the different columns? How many are non-numeric?.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to visualise the data. \n",
    "\n",
    "A possible way to visualise data quickly is to use a **boxplot**, which summarises the distribution of values on each dimension. \n",
    "\n",
    "Run the code below to generate boxplots. (Assumes that your dataframe is called 'df'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That plot is a bit too small and hard to read! Let's make it a bit bigger and rotate the labels ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.boxplot(figsize=(12, 8))\n",
    "plt.setp(ax.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different variables have different scales, so it doesn't make much sense to plot them all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Plot the boxplot separately for columns with comparable scales (e.g. remove the _FREQUENCY, _TRX, PRC_FULL_PAYMENT and TENURE columns and plot them separately).</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the boxplots: \n",
    "- The horizontal line within the box is the median value.\n",
    "- The box in the boxplot show the quartiles (the 25% and 75% percentiles) in the data, meaning that 50% of the data lies within the range of the box. \n",
    "- The whiskers extend 1.5 times the interquartile above/below the top/bottom of the box.\n",
    "- Points outside that range are often outliers and are shown on the plot as circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To produce a more comparable plot you could also use the StandardScalar routine to standardise the data on each dimension. We are going to use it, later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Compare a boxplot with a histogram for the same feature, e.g. the PURCHASES column.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2\n",
    "### Anomaly Detection\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now try to run the four types of anomaly detection system on this dataset.\n",
    "\n",
    "Before you can do that you'll need to do some preprocessing of the data to remove non-numeric columsn, and any rows containing null 'NaN' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: First of all, drop the non-numeric column.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown values are often coded as NaN in pandas dataframes. Run the following code to see if there are any in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Which columns contain NaN values?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code to show all lines containing a NaN value for the column called 'COLUMN_NAME': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['COLUMN_NAME'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Remove all rows containing NaN numbers or replace them with 'default' values.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Run the Eliptic Envelope (Robust Covariance) anomaly detection routine on the remaining data setting the contamination parameter to 0.01</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Eliptic Envelope (Robust Covariance) anomaly detection routine on the remaining data setting the contamination parameter to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_fraction =  # TODO\n",
    "y_pred =  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a warning, telling us that it should not happen.\n",
    "\n",
    "It might have to do with either\n",
    "- scaling of the data\n",
    "- some bad columns distribution\n",
    "\n",
    "I have performed the analysis and observed that the `'ONEOFF_PURCHASES'` causes the issue, thus I will just drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noNaN = df_noNaN.drop('ONEOFF_PURCHASES', axis=1)\n",
    "y_pred = EllipticEnvelope(contamination=outlier_fraction).fit_predict(df_noNaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: How many anomalies does it find?</b>\n",
    "</div>\n",
    "\n",
    "To find out how many anomalies it's found, we can print the count of the different values of y_pred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: The predicted anomalies have a value of -1. Print them out.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Visualise using boxplots the anomalous and non-anomalous datasets separately. Do the distributions look different?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can note that the values are generally much smaller for the non-anomalies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Plot the features BALANCE and PURCHASES against each other using a 2 dimensional scatter plot and color the dots differently based on whether they are considered anomalies or not. What do you conclude? </b>\n",
    "</div>\n",
    "\n",
    "Note: you will probably need to set the \"alpha\" parameter controllign transperancy to 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Repeat the analysis (i.e. generate and plot the predictions) with the other 3 anomaly detection techniques: One-class SVM, Isolation Forest and Local Outlier Factor. How well do they do?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-class SVM and LOF will be sensitive to scaling of the data, so we can use the StandardScalar to rescale the features so that they each have unit variance.\n",
    "We can then repeat the the anomaly detection after scaling and look for any change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Rescale the data using the StandardScaler.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_noNaN_scaled =  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Generate and plot the predictions.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: to perform the analysis, you could also plot the points labeled as outliers separately from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
