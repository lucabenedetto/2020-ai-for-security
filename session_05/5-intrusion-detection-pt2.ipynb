{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to remove the margins for the notebook, uncomment and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Network Intrusion Detection\n",
    "\n",
    "We use the same dataset as in the previous practical session.\n",
    "\n",
    "Our goal is to build a network intrusion detector, a predictive model capable of distinguishing between *bad* connections, called intrusions or attacks, and *good* normal connections.\n",
    "During last session we focused on **binary classification**, as we tried to distinguish between standard connections and attacks; in this session we will focus on **multi label classification**: that is, we will try not only to detect the attacks but also to correctly classify the type of attack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download instruction:\n",
    "- download the file kddcup.data.gz from [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) (**it is the same as last session, you can use the same file!**)\n",
    "- move it in the 'datasets' folder (or in some other folders, as long as you know the path)\n",
    "- extract the archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_intro.html) and [seaborn](https://seaborn.pydata.org/) for this notebook.\n",
    "\n",
    "If you use `pip` for managing packages, you can install them with:\n",
    "```\n",
    "    pip install xgboost\n",
    "    pip install seaborn\n",
    "```\n",
    "\n",
    "If you use Anaconda, these commands should work fine for you:\n",
    "```\n",
    "    conda install -c anaconda seaborn\n",
    "    conda install -c conda-forge xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Import libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    multilabel_confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Counter` is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. [DOC](https://docs.python.org/2/library/collections.html#collections.Counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual, you might have to change the value of these variables depending on the path you chose and your OS\n",
    "DATA_DIR = 'datasets/'\n",
    "FILENAME = 'kddcup.data.corrected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names obtained from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
    "header_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', \n",
    "    'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', \n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', \n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', \n",
    "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', \n",
    "    'dst_host_srv_rerror_rate', 'attack_type'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR+FILENAME, header=None, names=header_names, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>\n",
    "IMPORTANT:\n",
    "    \n",
    "The cell below reduces the size of the dataframe by sampling some of its elements. This is only done to work with a smaller amount of data. You can try to run the notebook without running this cell; if it crashes due to memory errors, come back here and rerun the notebook with less data.\n",
    "    \n",
    "If you still have troubles, there is a smaller version available on the same website.\n",
    "The file name is *kddcup.data_10_percent.gz*.\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "Let's quickly repeat part of the analysis we did last time, and use this as an opportunity to learn some visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows = %d\" % len(df.index))\n",
    "print(\"Number of columns = %d\" % len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = np.array(header_names)\n",
    "\n",
    "nominal_idx = [1, 2, 3]\n",
    "binary_idx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx))\n",
    "\n",
    "nominal_cols = col_names[nominal_idx].tolist()\n",
    "binary_cols = col_names[binary_idx].tolist()\n",
    "numeric_cols = col_names[numeric_idx].tolist()\n",
    "\n",
    "print(\"categorical attributes: \\n\", nominal_cols, \"\\n\")\n",
    "print(\"binary attributes: \\n\", binary_cols, \"\\n\")\n",
    "print(\"numeric attributes: \\n\", numeric_cols, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the distribution of the `protocol_type` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df.groupby('protocol_type').size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just printing the numbers, let's try to use the bar plot from `matplotlib` to plot the distribution.\n",
    "\n",
    "It works as follows:\n",
    "- initialize a \"figure\" and \"axes\" object with `plt.subplots()`\n",
    "- plot what you want in the specified axis (with `ax.<sth>`)\n",
    "- show the plot with `plt.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(tmp_df['protocol_type'], tmp_df[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With matplotlib, you can modify many aspects of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(tmp_df['protocol_type'], tmp_df[0])\n",
    "\n",
    "ax.set_xlabel('protocol type')\n",
    "ax.set_ylabel('Num. of connections')\n",
    "ax.set_title('Numer of connections for each protocol_type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you could also use `barh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.barh(tmp_df['protocol_type'], tmp_df[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the distribution of the `flag` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "Q: plot the distribution of <code>flag</code> using bar or barh from matplotlib\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "Documentation:\n",
    "- [bar](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.bar.html)\n",
    "- [barh](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.barh.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "Q: the values are of different magnitude, thus the plot is not really readable. Try to use <code>set_xscale</code> or <code>set_yscale</code> (depending on whether you are using barh or bar) to set 'log' scale and make it more readable.\n",
    "</b>\n",
    "</div>\n",
    "\n",
    "Hint: look at the format I used above to add the name of the axis and the title (i.e. `ax.set_xlabel()`).\n",
    "\n",
    "Documentation:\n",
    "- [set_xscale](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.set_xscale.html)\n",
    "- [set_yscale](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_yscale.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the distribution of the binary attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`matplotlib` lets you create different plots in the same `fig` object.\n",
    "You can do so by specifying different axes, as follows:\n",
    "- while initializing `fig` and `ax`, specify the dimension of the grid in `plt.subplots()` (e.g. `plt.subplots(2,2)` for a 2x2 grid)\n",
    "- in that case, `ax` is a matrix (or array) of axes and you have to specify the one you want to access while plotting (e.g. `ax[0][1].<sth>`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 8))\n",
    "\n",
    "tmp_df = df.groupby('land').size().reset_index().sort_values('land')\n",
    "ax[0][0].barh(tmp_df['land'], tmp_df[0])\n",
    "ax[0][0].set_yticks(tmp_df['land'])\n",
    "\n",
    "tmp_df = df.groupby('logged_in').size().reset_index().sort_values('logged_in')\n",
    "ax[0][1].barh(tmp_df['logged_in'], tmp_df[0])\n",
    "ax[0][1].set_yticks(tmp_df['logged_in'])\n",
    "\n",
    "tmp_df = df.groupby('root_shell').size().reset_index().sort_values('root_shell')\n",
    "ax[1][0].barh(tmp_df['root_shell'], tmp_df[0])\n",
    "ax[1][0].set_yticks(tmp_df['root_shell'])\n",
    "\n",
    "tmp_df = df.groupby('su_attempted').size().reset_index().sort_values('su_attempted')\n",
    "ax[1][1].barh(tmp_df['su_attempted'], tmp_df[0])\n",
    "ax[1][1].set_yticks(tmp_df['su_attempted'])\n",
    "\n",
    "tmp_df = df.groupby('is_host_login').size().reset_index().sort_values('is_host_login')\n",
    "ax[2][0].barh(tmp_df['is_host_login'], tmp_df[0])\n",
    "ax[2][0].set_yticks(tmp_df['is_host_login'])\n",
    "\n",
    "tmp_df = df.groupby('is_guest_login').size().reset_index().sort_values('is_guest_login')\n",
    "ax[2][1].barh(tmp_df['is_guest_login'], tmp_df[0])\n",
    "ax[2][1].set_yticks(tmp_df['is_guest_login'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also automate the creation of such plot without having to rewrite everything several times, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [\n",
    "    ['land', 'logged_in'],\n",
    "    ['root_shell', 'su_attempted'],\n",
    "    ['is_host_login', 'is_guest_login']\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(15, 10))\n",
    "\n",
    "for idx_x in range(3):\n",
    "    for idx_y in range(2):\n",
    "        column_name = cols_to_plot[idx_x][idx_y]\n",
    "        tmp_df = df.groupby(column_name).size().reset_index().sort_values(column_name)\n",
    "        ax[idx_x][idx_y].barh(tmp_df[column_name], tmp_df[0])\n",
    "        ax[idx_x][idx_y].set_yticks(tmp_df[column_name])\n",
    "        ax[idx_x][idx_y].set_xscale('log')\n",
    "        ax[idx_x][idx_y].set_title(column_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, from this plot, we can see that `is_host_login` is always 0 and `su_attempted` actually is not binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping each attack type to one category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_type'] = df.apply(lambda r: r['attack_type'][:-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['attack_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_df = df.groupby('attack_type').size().reset_index().sort_values(0, ascending=False)\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.barh(tmp_df['attack_type'], tmp_df[0])\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('attack_type')\n",
    "ax.set_xlabel('N. of samples (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = dict()\n",
    "category['benign'] = ['normal']\n",
    "\n",
    "with open(DATA_DIR+'training_attack_types.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        attack, cat = line.strip().split(' ')\n",
    "        if cat in category.keys():\n",
    "            category[cat].append(attack)\n",
    "        else:\n",
    "            category[cat] = [attack]\n",
    "\n",
    "attack_mapping = {v: k for k in category for v in category[k]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attack mapping:\")\n",
    "print(attack_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the actual mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_category'] = df.apply(lambda r: attack_mapping[r['attack_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_df = df.groupby('attack_category').size().reset_index().sort_values(0, ascending=False)\n",
    "display(tmp_df)\n",
    "\n",
    "# This example shows you that you can specify the color of each bar\n",
    "color = ['green' if category=='benign' else 'red' for category in tmp_df['attack_category']]\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(tmp_df['attack_category'], tmp_df[0], color=color)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('attack_category')\n",
    "ax.set_xlabel('N. of samples (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack2int = {x: idx for idx, x in enumerate(df['attack_category'].unique())}\n",
    "int2attack = {v: k for k, v in attack2int.items()}\n",
    "print(\"attack2int:\", attack2int)\n",
    "print(\"int2attack:\", int2attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: dummy variables\n",
    "\n",
    "We have some categorical variables. Thus, we have to converte them to one-hot encoded variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Create a new DataFrame encoding the categorical attributes with one hot encoding.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical feature into dummy variables with one-hot encoding\n",
    "df_one_hot ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Perform data split.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset up into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # TODO features, \n",
    "    # TODO labels, \n",
    "    test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell might take a while to run\n",
    "# also, if it crashes it might mean that you do not have enough memory available\n",
    "standard_scaler = StandardScaler().fit(X_train[numeric_cols])\n",
    "\n",
    "X_train[numeric_cols] = standard_scaler.transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = standard_scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Naive Bayes: define and train the model. Also, measure how long the training takes.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = # TODO: initialize the model\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO: fit the model\n",
    "print(\"elapsed time = %.2f\" % (time.time()-t0))\n",
    "\n",
    "y_pred_nb = # TODO: perform the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Naive Bayes: compute the accuracy.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_nb = # TODO: measure accuracy\n",
    "print(\"ACCURACY:\", accuracy_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Naive Bayes: Use the following code to print the multilabel confusion matrix and the heatmap.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class, the confusion matrix above behaves like a binary confusion matrix, considering the specified class vs all the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the predictions and the True values, you can also print the seaborn heatmat to understand how the predictions are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(zip(y_pred_nb, y_test))\n",
    "# create empty pandas DataFrame\n",
    "dff = pd.DataFrame(0, columns=np.unique(y_pred_nb) , index =np.unique(y_test))\n",
    "# insert counts in the DF\n",
    "for k,v in c.items():\n",
    "    dff[k[0]][k[1]] = v\n",
    "\n",
    "# plot the heatmap\n",
    "sns.heatmap(dff,annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows represent the **True** values, and the columns the **predicted** values.\n",
    "\n",
    "**The elements of the diagonal are the objects correctly classified.**\n",
    "\n",
    "**If you count the values on the row corresponding to, for example, `u2r`, you will obtain as total count the number of `u2r` attacks in the test set.\n",
    "On the other hand, if you count the values on the column corresponding to `u2r`, you will obtain as total count the number of test connections which were labelled as attacks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Decision Tree: define, train and test the model computing the evaluation metrics (*accuracy* and *multilabel_confusion_matrix*). Also, measure how long the training takes.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = # TODO: initialize the model\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO: fit the model\n",
    "print(\"elapsed time = %.2f\" % (time.time()-t0))\n",
    "\n",
    "y_pred_dt = # TODO: perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_dt = # TODO: measure accuracy\n",
    "print(\"ACCURACY:\", accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Using the code above, plot the heatmap.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can copy and paste the cell used for Naive Bayes, you just have to change one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Random Forest: define, train and test the model computing the evaluation metrics. Also, measure how long the training takes.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_rf = # TODO: initialize the model\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO: fit the model\n",
    "print(\"elapsed time = %.2f\" % (time.time()-t0))\n",
    "\n",
    "y_pred_rf = # TODO: perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_rf = # TODO: measure accuracy\n",
    "print(\"ACCURACY:\", accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Using the code above, plot the heatmap.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: SVM: define, train and test the model computing the evaluation metrics. Also, measure how long the training takes.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = # TODO: initialize the model\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO: fit the model\n",
    "print(\"elapsed time = %.2f\" % (time.time()-t0))\n",
    "\n",
    "y_pred_svc = # TODO: perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svc = # TODO: measure accuracy\n",
    "print(\"ACCURACY:\", accuracy_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Using the code above, plot the heatmap.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Run your first neural net</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nn = MLPClassifier(hidden_layer_sizes=(200, ), activation='relu')\n",
    "\n",
    "# train the classifier\n",
    "t0 = time.time()\n",
    "# TODO: fit the model\n",
    "print(\"elapsed time = %.2f\" % (time.time()-t0))\n",
    "\n",
    "y_pred_nn = # TODO: perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_nn = # TODO: measure accuracy\n",
    "print(\"ACCURACY:\", accuracy_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Using the code above, plot the heatmap.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: XGBoost: define, train and test the model computing the evaluation metrics. Also, measure how long the training takes.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is not part of sklearn but its usage is pretty much the same as the models from sklearn. The class of the classifier is `XGBClassifier()`.\n",
    "\n",
    "[Documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html) (search for class xgboost.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = # TODO: initialize the XGBClassifier() object (as starter, use default arguments)\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO: fit the model\n",
    "print(\"elapsed time = %.2f\" % (time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = # TODO: perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_xgb = # TODO: measure accuracy\n",
    "print(\"ACCURACY:\", accuracy_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Using the code above, plot the heatmap.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Which model do you think works best? Why?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How many features does our model get as input?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot assume that every feature is as important as the others. Some features might be very useful, some other features might even worsen the prediction!\n",
    "\n",
    "A way to measure the \"importance\" of each feature is given by some models. For instance:\n",
    "- the `feature_importances_` attribute of Random Forests\n",
    "- the `plot_importance` method of xgboost\n",
    "- the `feature_importances_` attribute of xgboost classifiers\n",
    "\n",
    "Still, we have to be careful as we cannot blindly trust these values.\n",
    "\n",
    "Let's try to work on the `plot_importance` from xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "asd = xgb.plot_importance(model, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Which is the importance of the features accordingly to the XGBoost model trained above? Which are the most important features? Which are the least important features? And how big is the difference between their importance?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ANS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "    Q: \n",
    "    Use the following line to store the importances in an array.\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "    Q: \n",
    "    Use the following line to create a dict that maps each column to its importance (as computed by the XGBoost model).\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_dict = dict()\n",
    "for idx in range(len(X_train.columns)):\n",
    "    feature_importances_dict[X_train.columns[idx]] = importances[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Focus on the least important features: look at their distribution, their max values, etc. Is there anything strange with them?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Focus on the most important features: look at their distribution, their max values, etc. Is there anything strange with them?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Try to remove the least important features (if you want, you can try removing different numbers of features) and see how the performance changes. Try also removing the most important features. \n",
    "    \n",
    "For future work, also observe how the features' importance changes in each situation, and do not limit this analysis to the XGBoost, but try to do the same with the other models as well.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the RandomForest has a `feature_importances_` attribute, that returns the importance of each feature. You can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_).\n",
    "**However, it is important not to trust blindly the values returned by such attribute**, as it only represents \"the (normalized) total reduction of the criterion brought by that feature\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
