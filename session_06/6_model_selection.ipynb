{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Model and hyper parameters selection\n",
    "\n",
    "In this session, we will focus on the techniques for model choice and hyperparameter tuning for classification.\n",
    "Similar techniques can be used for regression tasks as well.\n",
    "\n",
    "We are going to use two datasets which we have already explored in previous sessions: \n",
    "- the dataset on fraud detection (the `payment_fraud.csv` file in the datasets folder)\n",
    "- the kdd dataset on intrusion detection ([website](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)); you can either re-download it from the website or copy-n-paste it from the folder of one of the previous sessions. Remember, if you download it again, that you need the *kdd.data.gz* file (or the *10percent*), which is a compressed archive (you have to extract it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- [0. imports](#0_imports)\n",
    "- [1. fraud detection](#1_fraud_detection)\n",
    "    - [1.1 load dataset](#1.1_load_dataset)\n",
    "    - [1.2 analysis](#1.2_analysis)\n",
    "    - [1.3 data preparation](#1.3_data_preparation)\n",
    "    - [1.4 training and evaluation](#1.4_training_and_evaluation)\n",
    "    - [1.5 analysis of different parameters](#1.5_analysis_of_different_parameters)\n",
    "- [2. Introduction to GridSearchCV and RandomizedSearchCV](#2_Introduction_to_GridSearchCV_and_RandomizedSearchCV)\n",
    "- [3. Intrusion detection](#3_intrusion_detection)\n",
    "    - [3.1 load dataset](#3.1_load_dataset)\n",
    "    - [3.2 map each attack to corresponding category](#3.2_mapping_attack_to_category)\n",
    "    - [3.3 subsampling](#3.3_subsampling)\n",
    "    - [3.4 data analysis](#3.4_data_analysis)\n",
    "    - [3.5 data preparation](#3.5_data_preparation)\n",
    "    - [3.6 training and evaluation](#3.6_training_and_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# methods for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# distribution probabilities\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_fraud_detection\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1_load_dataset\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets'\n",
    "filename_fraud = 'payment_fraud.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2_analysis\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = pd.read_csv(os.path.join(data_dir, filename_fraud))\n",
    "df_fraud.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Print the number of rows.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Print the number of occurrences of all the possible values of paymentMethod and paymentMethodAgeDays.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paymentMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Plot the distribution of the possible values of numItems.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Plot the distribution of the accountAgeDays column.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Plot the distribution of the localTime column.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3_data_preparation\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "You might remember that we had found out that all the fraudulent transactions were identified by the accountAgeDays attribute (see plots below for demonstration). Thus, for the sake of this session, we will drop such column in order to make the dataset more challenging.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell and the next one have the only purpose of showing that accountAgeDays can be use to perfectly distinguish between standard and malicious transactions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4), sharex=True)\n",
    "\n",
    "bins = np.arange(0, 2001, 50)\n",
    "ax[0].hist(df_fraud[df_fraud['label']==0]['accountAgeDays'], bins=bins, color='g')\n",
    "ax[1].hist(df_fraud[df_fraud['label']==1]['accountAgeDays'], bins=bins, color='r')\n",
    "\n",
    "for idx in [0, 1]:\n",
    "    ax[idx].set_title(\"Distribution of accountAgeDays for '%d' entries (i.e. %s)\" % (idx, \"'good'\" if idx==0 else \"'fraud'\"))\n",
    "    ax[idx].grid(axis='y')\n",
    "    ax[idx].set_ylabel('Num. of transactions')\n",
    "    ax[idx].set_xlabel('accountAgeDays')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4), sharex=True)\n",
    "\n",
    "bins = np.arange(0, 10, 1)\n",
    "ax[0].hist(df_fraud[(df_fraud['label']==0)&(df_fraud['accountAgeDays']<5)]['accountAgeDays'], bins=bins, color='g')\n",
    "ax[1].hist(df_fraud[(df_fraud['label']==1)&(df_fraud['accountAgeDays']<5)]['accountAgeDays'], bins=bins, color='r')\n",
    "\n",
    "for idx in [0, 1]:\n",
    "    ax[idx].grid(axis='y')\n",
    "    ax[idx].set_title(\"Focusing on the [0, 10] range\")\n",
    "    ax[idx].set_xticks(bins)\n",
    "    ax[idx].set_ylabel('Num. of transactions')\n",
    "    ax[idx].set_xlabel('accountAgeDays')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = df_fraud.drop('accountAgeDays', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How unbalanced is the dataset?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "It is very unbalanced. There are several techniques to address this, and you will see them in the classroom. Now we will be using the one which is probably the simplest: undersampling.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_fraud[df_fraud['label']==1]\n",
    "df_0 = df_fraud[df_fraud['label']==0].sample(len(df_1)*2)\n",
    "\n",
    "df_fraud = pd.concat([df_0, df_1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How unbalanced is the dataset now?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Which are the numeric columns? Which are the categorical columns?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Define two variables (numeric_cols and nominal_cols) containing a list of numerical and nominal attributes, respectively.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_cols = # TODO\n",
    "numeric_cols = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Perform one hot encoding.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Perform train-test split.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Perform scaling.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = # TODO\n",
    "\n",
    "X_train[numeric_cols] = # TODO\n",
    "X_test[numeric_cols] = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4_training_and_evaluation\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Define, train, and evaluate a KNeighborsClassifier. Use K=1</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "clf = # TODO\n",
    "\n",
    "# train the classifier\n",
    "t0 = time.time()\n",
    "# TODO\n",
    "print(\"elapsed time = %.5f\" % (time.time()-t0))\n",
    "\n",
    "# perform the prediction\n",
    "y_pred = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = # TODO\n",
    "precision = # TODO\n",
    "recall = # TODO\n",
    "print(\"accuracy\", accuracy)\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Try now with K=2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "clf = # TODO\n",
    "\n",
    "# train the classifier\n",
    "t0 = time.time()\n",
    "# TODO\n",
    "print(\"elapsed time = %.5f\" % (time.time()-t0))\n",
    "\n",
    "# perform the prediction\n",
    "y_pred = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = # TODO\n",
    "precision = # TODO\n",
    "recall = # TODO\n",
    "print(\"accuracy\", accuracy)\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How did the performance change? Can you infer anything from it?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ANS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5_analysis_of_different_parameters\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Run the following cell. What does it do?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_n_neighbors = np.arange(1, 200, 1, dtype=np.int16)\n",
    "\n",
    "list_accuracy = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "list_training_time = []\n",
    "\n",
    "for n in list_n_neighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=n)\n",
    "\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    list_training_time.append(time.time()-t0)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    list_accuracy.append(accuracy_score(y_pred, y_test))\n",
    "    list_precision.append(precision_score(y_pred, y_test))\n",
    "    list_recall.append(recall_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ANS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "ax[0][0].plot(list_accuracy, c='tab:blue')\n",
    "ax[0][0].set_title('KNeighborsClassifier: accuracy')\n",
    "ax[0][0].set_ylabel('accuracy')\n",
    "ax[0][0].set_xlabel('Num. of neighbors (K)')\n",
    "\n",
    "ax[0][1].plot(list_precision, c='tab:orange')\n",
    "ax[0][1].set_title('KNeighborsClassifier: precision')\n",
    "ax[0][1].set_ylabel('precision')\n",
    "ax[0][1].set_xlabel('Num. of neighbors (K)')\n",
    "\n",
    "ax[1][0].plot(list_recall, c='tab:green')\n",
    "ax[1][0].set_title('KNeighborsClassifier: recall')\n",
    "ax[1][0].set_ylabel('recall')\n",
    "ax[1][0].set_xlabel('Num. of neighbors (K)')\n",
    "\n",
    "ax[1][1].plot(list_training_time, c='tab:red')\n",
    "ax[1][1].set_title('KNeighborsClassifier: training_time')\n",
    "ax[1][1].set_ylabel('training_time')\n",
    "ax[1][1].set_xlabel('Num. of neighbors (K)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Try to analyse the plots above. Do you see any patterns? Is there a choice of parameters that looks particularly effective in your opinion?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ANS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Let's repeat the same study, but considering SVM and its C parameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_c = np.arange(0.1, 5.1, 0.1, dtype=np.float)\n",
    "\n",
    "list_accuracy = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "list_training_time = []\n",
    "\n",
    "for c in list_c:\n",
    "    clf = LinearSVC(C=c, max_iter=20000)\n",
    "\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    list_training_time.append(time.time()-t0)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    list_accuracy.append(accuracy_score(y_pred, y_test))\n",
    "    list_precision.append(precision_score(y_pred, y_test))\n",
    "    list_recall.append(recall_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "ax[0][0].plot(list_accuracy, c='tab:blue')\n",
    "ax[0][0].set_title('LinearSVC: accuracy')\n",
    "ax[0][0].set_ylabel('accuracy')\n",
    "ax[0][0].set_xlabel('Regularization parameter (C)')\n",
    "\n",
    "ax[0][1].plot(list_precision, c='tab:orange')\n",
    "ax[0][1].set_title('LinearSVC: precision')\n",
    "ax[0][1].set_ylabel('precision')\n",
    "ax[0][1].set_xlabel('Regularization parameter (C)')\n",
    "\n",
    "ax[1][0].plot(list_recall, c='tab:green')\n",
    "ax[1][0].set_title('LinearSVC: recall')\n",
    "ax[1][0].set_ylabel('recall')\n",
    "ax[1][0].set_xlabel('Regularization parameter (C)')\n",
    "\n",
    "ax[1][1].plot(list_training_time, c='tab:red')\n",
    "ax[1][1].set_title('LinearSVC: training_time')\n",
    "ax[1][1].set_ylabel('training_time')\n",
    "ax[1][1].set_xlabel('Regularization parameter (C)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Try to analyse the plots above. Do you see any patterns? Is there a choice of parameters that looks particularly effective in your opinion?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ANS</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_Introduction_to_GridSearchCV_and_RandomizedSearchCV\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1_GridSearchCV\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "`GridSearchCV` lets you define a grid of hyperparameters to evaluate with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First of all, you have to define the model to use, in this case LinearSVC but it could be any sklearn model (DecisionTreeClassifier, RandomForestClassifier, etc.)\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# 2. define a Pipeline, in this case it is made of only one element\n",
    "pipe = Pipeline(steps=[('clf', classifier)])\n",
    "\n",
    "# 3. parameters of the model in the pipeline can be set using `'__'` separated parameter names; e.g. `'clf__C'` is a list of values for the C attribute of the classifier\n",
    "param_grid = {\n",
    "    'clf__C': np.arange(0.1, 5.1, 0.1, dtype=np.float),\n",
    "    'clf__max_iter': [20000],\n",
    "}\n",
    "\n",
    "# 4. performs cross validation for each of the parameters set above. `cv=3` means that I perform 3-fold cross validation\n",
    "search = GridSearchCV(pipe, param_grid, iid=False, cv=3, verbose=True)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# 5. once the training is done, you can show the parameters of the best performing model\n",
    "print(search.best_params_)\n",
    "\n",
    "# 6. and you can also retrieve it in order to perform the final test on the test set\n",
    "trained_clf = search.best_estimator_.get_params()['clf']\n",
    "trained_clf\n",
    "\n",
    "# 7. prediction and evaluation is done as usual\n",
    "y_pred = trained_clf.predict(X_test)\n",
    "\n",
    "print(\"accuracy\", accuracy_score(y_pred, y_test))\n",
    "print(\"precision\", precision_score(y_pred, y_test))\n",
    "print(\"recall\", recall_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2_RandomizedSearchCV\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RandomizedSearchCV` lets you define distribution of hyperparameters to evaluate with cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used in the same way as `GridSearchCV`, but there is a difference in how the parameters are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC()\n",
    "\n",
    "pipe = Pipeline(steps=[('clf', classifier)])\n",
    "\n",
    "# 3. parameters are not a grid. They are probability distributions\n",
    "distributions = {\n",
    "    'clf__C': uniform(0, scale=5),\n",
    "    'clf__max_iter': [20000],\n",
    "}\n",
    "\n",
    "# 4. performs cross validation, parameters are taken from the distributions defined above\n",
    "search = RandomizedSearchCV(pipe, distributions, n_iter=50, cv=3, verbose=True)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "trained_clf = search.best_estimator_.get_params()['clf']\n",
    "trained_clf\n",
    "\n",
    "y_pred = trained_clf.predict(X_test)\n",
    "\n",
    "print(\"accuracy\", accuracy_score(y_pred, y_test))\n",
    "print(\"precision\", precision_score(y_pred, y_test))\n",
    "print(\"recall\", recall_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3_intrusion_detection\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1_load_dataset\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'kddcup.data.corrected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names obtained from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
    "header_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', \n",
    "    'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', \n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', \n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', \n",
    "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', \n",
    "    'dst_host_srv_rerror_rate', 'attack_type'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, filename), header=None, names=header_names, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try to run the cells of section 3.2 *without* performing the sampling at this point.\n",
    "If your machine crashes due to memory issues, then come back here, uncomment this line, and possibly change the portion of the DF to keep.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2_mapping_attack_to_category\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove final \".\" from the attack type\n",
    "df['attack_type'] = df.apply(lambda r: r['attack_type'][:-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of attacks\n",
    "len(df['attack_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of attacks\n",
    "tmp_df = df.groupby('attack_type').size().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.barh(tmp_df['attack_type'], tmp_df[0])\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('attack_type')\n",
    "ax.set_xlabel('N. of samples (log scale)')\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary to map attack types to attack categories\n",
    "category = dict()\n",
    "category['benign'] = ['normal']\n",
    "\n",
    "with open(os.path.join(data_dir, 'training_attack_types.txt'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        attack, cat = line.strip().split(' ')\n",
    "        if cat in category.keys():\n",
    "            category[cat].append(attack)\n",
    "        else:\n",
    "            category[cat] = [attack]\n",
    "\n",
    "attack_mapping = {v: k for k in category for v in category[k]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the mapping\n",
    "df['attack_category'] = df.apply(lambda r: attack_mapping[r['attack_type']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define dictionaries that map an integer to an attack category and vice versa\n",
    "attack2int = {x: idx for idx, x in enumerate(df['attack_category'].unique())}\n",
    "int2attack = {v: k for k, v in attack2int.items()}\n",
    "print(\"attack2int:\", attack2int)\n",
    "print(\"int2attack:\", int2attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of attack categories\n",
    "tmp_df = df.groupby('attack_category').size().reset_index().sort_values(0, ascending=False)\n",
    "display(tmp_df)\n",
    "\n",
    "color = ['green' if category=='benign' else 'red' for category in tmp_df['attack_category']]\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(tmp_df['attack_category'], tmp_df[0], color=color)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('attack_category')\n",
    "ax.set_xlabel('N. of samples (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3_subsampling\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In previous notebooks, we had always performed a random sampling, ignoring the labels. Now we are going to perform a \"targeted sampling\": that is, we are going to reduce the size of the dataset by removing only rows belonging to the most frequent categories, so that we do not discard entries for the least frequent categories.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have problems running the notebook, you could further reduce the dataframe size\n",
    "df_dos = df[df['attack_category']=='dos'].sample(n=10**6)\n",
    "df_not_dos = df[df['attack_category']!='dos']\n",
    "\n",
    "df = pd.concat([df_dos, df_not_dos], axis=0).sample(frac=1.)  # the final frac=1. is done to reshuffle the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df.groupby('attack_category').size().reset_index().sort_values(0, ascending=False)\n",
    "display(tmp_df)\n",
    "\n",
    "color = ['green' if category=='benign' else 'red' for category in tmp_df['attack_category']]\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(tmp_df['attack_category'], tmp_df[0], color=color)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('attack_category')\n",
    "ax.set_xlabel('N. of samples (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4_data_analysis\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows = %d\" % len(df.index))\n",
    "print(\"Number of columns = %d\" % len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = np.array(header_names)\n",
    "\n",
    "nominal_idx = [1, 2, 3]\n",
    "binary_idx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx))\n",
    "\n",
    "nominal_cols = col_names[nominal_idx].tolist()\n",
    "binary_cols = col_names[binary_idx].tolist()\n",
    "numeric_cols = col_names[numeric_idx].tolist()\n",
    "\n",
    "print(\"categorical attributes: \\n\", nominal_cols, \"\\n\")\n",
    "print(\"binary attributes: \\n\", binary_cols, \"\\n\")\n",
    "print(\"numeric attributes: \\n\", numeric_cols, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df.groupby('protocol_type').size().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(tmp_df['protocol_type'], tmp_df[0])\n",
    "\n",
    "ax.set_title('Numer of connections for each protocol_type')\n",
    "ax.grid(axis='x')\n",
    "ax.set_ylabel('protocol_type')\n",
    "ax.set_xlabel('N. of samples (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df.groupby('flag').size().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(tmp_df['flag'], tmp_df[0])\n",
    "ax.set_xscale('log')\n",
    "ax.grid(axis='x')\n",
    "ax.set_title('Numer of connections for each flag')\n",
    "ax.set_ylabel('flag')\n",
    "ax.set_xlabel('N. of samples (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If you compare the two plots above with the corresponding ones from last week you can see that the subsampling heavily affected the distribution. This is not necessarily a problem, but you should be aware of it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5_data_preparation\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Create a new DataFrame encoding the categorical attributes with one hot encoding.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Split in train and test set.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Perform scaling.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = # TODO\n",
    "\n",
    "X_train[numeric_cols] = # TODO\n",
    "X_test[numeric_cols] = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6_training_and_evaluation\n",
    "[Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "Q: Use the GridSearchCV method seen above to train a DecisionTreeClassifier(), then evaluate the best performing model. Use the following parameters:\n",
    "\n",
    "</b>\n",
    "    \n",
    "    - max_leaf_nodes: [10, None];\n",
    "    - max_depth: [2, 5, 10]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = # TODO\n",
    "\n",
    "# TODO Pipeline\n",
    "\n",
    "# TODO param_grid\n",
    "\n",
    "search = # TODO\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO fit\n",
    "print(\"Elapsed time = %.2f\" % (time.time()-t0))\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "trained_clf = # TODO get best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY:\", # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Search for the differences between this function and the corresponding one defined in the previous session.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(y_pred, y_test, vmax=None, vmin=None, cmap=None):\n",
    "    c = Counter(zip(y_pred, y_test))\n",
    "    dff = pd.DataFrame(0, columns=np.unique(y_pred) , index =np.unique(y_test))\n",
    "    for k,v in c.items():\n",
    "        dff[k[0]][k[1]] = v\n",
    "    sns.heatmap(dff,annot=True, fmt=\"d\", vmax=vmax, vmin=vmin, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(y_pred, y_test, vmax=2000, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "for attack_type in set(attack_mapping.values()):\n",
    "    print(\"%10s -> \" % attack_type, clf_report[attack_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "Q: Use the RandomizedSearchCV method seen above to train a DecisionTreeClassifier(), then evaluate the best performing model. Use the following parameters:\n",
    "\n",
    "</b>\n",
    "    \n",
    "    - max_depth: randint(1, 20)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = # TODO\n",
    "\n",
    "# TODO Pipeline\n",
    "\n",
    "# TODO distributions\n",
    "\n",
    "search = # TODO RandomizedSearchCV\n",
    "# TODO fit\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "trained_clf = # TODO get best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY:\", # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(y_pred, y_test, vmax=2000, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "for attack_type in set(attack_mapping.values()):\n",
    "    print(\"%10s -> \" % attack_type, clf_report[attack_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "Q: Even if XGBClassifier is not part of sklearn, you can use it withing GridSearchCV and RandomizedSearchCV. Train and evaluate XGBClassifier with GridSearch using the following parameters:\n",
    "\n",
    "</b>\n",
    "    \n",
    "    - learning_rate: np.arange(0.1, 0.6, 0.2);\n",
    "    - n_estimators: [50, 100]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = # TODO\n",
    "\n",
    "pipe = # TODO\n",
    "\n",
    "param_grid = # TODO\n",
    "\n",
    "search = # TODO\n",
    "t0 = time.time()\n",
    "# TODO fit\n",
    "print(\"Elapsed time = %.2f\" % (time.time()-t0))\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "# TODO get best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY:\", # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(y_pred, y_test, vmax=2000, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "for attack_type in set(attack_mapping.values()):\n",
    "    print(\"%10s -> \" % attack_type, clf_report[attack_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Now it's your turn. Experiment with different models, hyperparameters and try to find the best performing model using cross validation with GridSearchCV and/or RandomizedSearchCV.</b>\n",
    "    \n",
    "Some things you might want to try:\n",
    "    \n",
    "- work on the features: are all of them important? How are they distributed? Does the model gets better if you remove some of the features? etc...\n",
    "\n",
    "- work on the data: is the StandardScaler the best scaler to use? Are the chosen parameters the ones leading to the best result?\n",
    "\n",
    "- try different models and different hyper parameters for each model (you cannot try \"every\" possible combination, try to pick wisely). For instance, you first do a large grained search and. after that, a fine grained search \"around\" the best parameters. The SearchCV might even take several hours (or days) to run if you set too many parameters.\n",
    "</div>\n",
    "\n",
    "Remember: \n",
    "- *one perfect model* to solve this problem does not exist, but by approaching the problem in the correct way, you can get better and better models.\n",
    "- when performing the final evaluation, look at all the 5 classes and not only at the overall accuracy!\n",
    "\n",
    "After you finish the notebook, take a look at this [link](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html)\n",
    "\n",
    "In the second Section (\"Face recognition with eigenfaces\"), it presents an example of how to use the techniques that you have seen so far in a more challenging problem (face recognition), which is also very relevant from a security perspective, since face recognition is often used for authentication on hand-held devices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
