{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Network Intrusion Detection\n",
    "\n",
    "The dataset used in this notebook originally comes from a KDD competition held several years ago.\n",
    "\n",
    "Here you can find the original task description given to the competition participants: [task description](http://kdd.ics.uci.edu/databases/kddcup99/task.html)\n",
    "\n",
    "The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between *bad* connections, called intrusions or attacks, and *good* normal connections. \n",
    "The database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download instruction:\n",
    "- download the file kddcup.data.gz from [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)\n",
    "- move it in the 'datasets' folder (or in some other folders, as long as you know the path)\n",
    "- extract the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual, you might have to change this depending on the path you chose and your operating system\n",
    "DATA_DIR = 'datasets'\n",
    "FILENAME = 'kddcup.data.corrected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names obtained from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
    "header_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', \n",
    "    'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', \n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', \n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', \n",
    "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', \n",
    "    'dst_host_srv_rerror_rate', 'attack_type'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, FILENAME), header=None, names=header_names, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: What is the effect of setting <i>header=None</i>?</b>\n",
    "</div>\n",
    "\n",
    "Hint: take a look at the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: What is the effect of setting <i>names=header_names</i>?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: How many rows in the dataframe?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>\n",
    "IMPORTANT:\n",
    "    \n",
    "The cell below reduces the size of the dataframe by sampling some of its elements. This is only done to work with a smaller amount of data. You can try to run the notebook without running this cell; if it crashes due to memory errors, come back here and rerun the notebook with less data.\n",
    "    \n",
    "If you still have troubles, there is a smaller version available on the same website.\n",
    "The file name is *kddcup.data_10_percent.gz*.\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: How many rows in the dataframe after sampling?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial analysis of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Display the first 5 rows of the dataframe</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How many columns does the original dataframe have?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How many FEATURES?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Are there any categorical variables?</b>\n",
    "</div>\n",
    "\n",
    "Hint: use the [.info()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the dataset and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = np.array(header_names)\n",
    "\n",
    "nominal_idx = [1, 2, 3]\n",
    "binary_idx = [6, 11, 13, 14, 20, 21]\n",
    "numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx))\n",
    "\n",
    "nominal_cols = col_names[nominal_idx].tolist()\n",
    "binary_cols = col_names[binary_idx].tolist()\n",
    "numeric_cols = col_names[numeric_idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: What is the difference between <i>col_names</i> and <i>header_names</i>?</b>\n",
    "</div>\n",
    "\n",
    "[hint](https://docs.python.org/3/library/functions.html#type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How many distinct values exist for the categorical variables?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: If you you solved the previous question with more than two lines of code, do the same but try to use only *two* lines of code.</b>\n",
    "</div>\n",
    "\n",
    "hint: remember the `for` loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What are the possible values of the categorical variables?</b>\n",
    "</div>\n",
    "\n",
    "Try to use only **two** lines of code to print all the possible values of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Which is the maximum duration, minimun duration and average duration of the entries in the dataframe?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How many entries are 'root_shell' and how many aren't?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Count the number of entries for each 'protocol_type'</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Which is the most frequent 'service'?</b> Try to write a cell that prints the name of the most frequent service as a string.\n",
    "</div>\n",
    "\n",
    "Hint: remember the [groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) and [sort_values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping each attack type to one category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this, we are going to use the file *training_attack_types.txt*, which maps each of the attacks in the original dataset to 1 category.\n",
    "\n",
    "If you have downloaded the zip archive from beep, you have this file in the dataset folder; otherwise, you can get it from the github repo or from [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How many different 'attack_types' are in the dataframe?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Display the number of occurrences of each attack type</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What does the following cell do?</b>\n",
    "</div>\n",
    "\n",
    "- hint: display the dataframe before and after performing this operation and look at it\n",
    "- [hint](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_type'] = df.apply(lambda r: r['attack_type'][:-1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = dict()\n",
    "category['benign'] = ['normal']\n",
    "\n",
    "TRAINING_ATTACK_TYPES_FILENAME = 'training_attack_types.txt'\n",
    "with open(os.path.join(DATA_DIR, TRAINING_ATTACK_TYPES_FILENAME), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        attack, cat = line.strip().split(' ')\n",
    "        if cat in category.keys():\n",
    "            category[cat].append(attack)\n",
    "        else:\n",
    "            category[cat] = [attack]\n",
    "\n",
    "attack_mapping = {v: k for k in category for v in category[k]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What is 'attack_mapping'?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How many categories of attacks are there? What are their names?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the actual mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_category'] = df.apply(lambda r: attack_mapping[r['attack_type']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Count the number of occurrences of each category</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: dummy variables\n",
    "\n",
    "We have some categorical variables. Thus, we have to converte them to one-hot encoded variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Create a new DataFrame encoding the categorical attributes with one hot encoding.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical feature into dummy variables with one-hot encoding\n",
    "df_one_hot = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset up into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # TODO X, \n",
    "    # TODO Y, \n",
    "    test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell might take a while to run\n",
    "# also, if it crashes it might mean that you do not have enough memory available\n",
    "standard_scaler = StandardScaler().fit(X_train[numeric_cols])\n",
    "\n",
    "X_train[numeric_cols] = standard_scaler.transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = standard_scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation: converting label to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What does the following cell do?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = y_train.apply(lambda x: 0 if x == 'benign' else 1)\n",
    "y_test_bin = y_test.apply(lambda x: 0 if x == 'benign' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, find the best model in detecting whether an entry is malicious or not (i.e. use the binary label). We will work on multi-label classification in the next notebook.\n",
    "\n",
    "We will try with different models and we will also have a look at how we can perform cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy Example: how to measure elapsed time\n",
    "a = 5\n",
    "t0 = time.time()\n",
    "for value in range(10):\n",
    "    a *= value\n",
    "print(\"Elapsed time:\", time.time() - t0, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Q: Define a function named <code>print_eval_metrics</code> that receives as input the predictions and the true labels and *prints* accuracy, precistion, recall and the confusion matrix.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_metrics(predicted_labels, true_labels):\n",
    "    accuracy = # TODO\n",
    "    precision = # TODO\n",
    "    recall = # TODO\n",
    "    cm = # TODO\n",
    "    \n",
    "    # Do not change the code below\n",
    "    print(\"ACCURACY:  %.5f\" % accuracy)\n",
    "    print(\"PRECISION: %.5f\" % precision)\n",
    "    print(\"RECALL:    %.5f\" % recall)\n",
    "    print(\"CONFUSION MATRIX:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Define, train, and evaluate the models specified in the following cells. For all of them, measure the training time as well.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Naive Bayes.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_nb = # TODO define the classifier\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO fit the classifier\n",
    "elapsed_time = # TODO\n",
    "print(\"training time = %.2f\" % elapsed_time)\n",
    "\n",
    "# TODO perform the prediction\n",
    "y_pred_nb = \n",
    "\n",
    "# evaluate\n",
    "print_eval_metrics(y_pred_nb, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Random Forest</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = # TODO define the classifier\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO fit the classifier\n",
    "elapsed_time = # TODO\n",
    "print(\"training time = %.2f\" % elapsed_time)\n",
    "\n",
    "# TODO perform the prediction\n",
    "y_pred_rf = \n",
    "\n",
    "# evaluate\n",
    "print_eval_metrics(y_pred_rf, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Decision Tree</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = # TODO define the classifier\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO fit the classifier\n",
    "elapsed_time = # TODO\n",
    "print(\"training time = %.2f\" % elapsed_time)\n",
    "\n",
    "# TODO perform the prediction\n",
    "y_pred_dt = \n",
    "\n",
    "# evaluate\n",
    "print_eval_metrics(y_pred_dt, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>k-nearest neighbors (this might take a while) </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = # TODO define the classifier\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO fit the classifier\n",
    "elapsed_time = # TODO\n",
    "print(\"training time = %.2f\" % elapsed_time)\n",
    "\n",
    "# TODO perform the prediction\n",
    "y_pred_knn = \n",
    "\n",
    "# evaluate\n",
    "print_eval_metrics(y_pred_knn, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>\n",
    "SVM; try two versions of the SVM: \n",
    "    \n",
    "    i) kernel='rbf' \n",
    "    ii) kernel='linear'\n",
    "    \n",
    "(this might take a while)\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = # TODO define the classifier with kernel='rbf'\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO fit the classifier\n",
    "elapsed_time = # TODO\n",
    "print(\"training time = %.2f\" % elapsed_time)\n",
    "\n",
    "# TODO perform the prediction\n",
    "y_pred_svc = \n",
    "\n",
    "# evaluate\n",
    "print_eval_metrics(y_pred_svc, y_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = # TODO define the classifier with kernel='linear'\n",
    "\n",
    "t0 = time.time()\n",
    "# TODO fit the classifier\n",
    "elapsed_time = # TODO\n",
    "print(\"training time = %.2f\" % elapsed_time)\n",
    "\n",
    "# TODO perform the prediction\n",
    "y_pred_svc = \n",
    "\n",
    "# evaluate\n",
    "print_eval_metrics(y_pred_svc, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: The following cell performs cross validation to find the best performing configuration of the Decision Tree Classifier. Run it as it is and then try to change the parameters to improve the performance.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    param_grid={'max_depth': [2, 3, 4, 5, 10, 15, 20, 25, 50, None]}, \n",
    "    cv=10\n",
    ")\n",
    "best_clf = clf.fit(X_train, y_train_bin)\n",
    "\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "print_eval_metrics(y_pred, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Which model do you think works best? Why do you say so?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "ANS\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
